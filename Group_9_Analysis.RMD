---
title: "Group_9_Analysis"
author: "Brent Strong, Enyu Li, Haotian Wang, Honjin Ren, Mu He"
date: "3/7/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, message = FALSE, warning = FALSE)
```

#Setup and Data Import

```{r libraries, echo = FALSE}

#Load necessary libraries for the data exploration and analysis

library(tidyverse)
library(moderndive)
library(skimr)
library(kableExtra)
library(gridExtra)
library(broom)
library(olsrr)
library(gapminder)
library(sjPlot)
library(stats)
library(jtools)
library(MASS)
library(janitor)
library(ggplot2)
library(caret)
library(lme4)
library(ROCR)
```

```{r import, echo = FALSE}

#Read in data from github and abbreviate certain country names.

analysis <- read_csv("https://raw.githubusercontent.com/brent-strong/DAS2022-Group-09/main/dataset9.csv")

# glimpse(analysis)

for(i in 1:nrow(analysis)){
  if(str_detect(analysis$country_of_origin[i], "Puerto Rico")){
    analysis$country_of_origin[i] <- "Puerto Rico"
  }
  if(str_detect(analysis$country_of_origin[i], "Hawaii")){
    analysis$country_of_origin[i] <- "Hawaii"
  }
  if(str_detect(analysis$country_of_origin[i], "Tanzania")){
    analysis$country_of_origin[i] <- "Tanzania"
  }
  else{
    analysis$country_of_origin[i] <- analysis$country_of_origin[i]
  }
}

# View(analysis)

# Examine the proportion of batches that are good:

# mean(analysis$Qualityclass=="Good")
```

#Exploratory Data Analysis

```{r, eval = FALSE, echo=FALSE, eval = TRUE}

#Create table of summary statistics for each of the continuous variables. 

my_skim2 <- skim_with(numeric = sfl(hist = NULL))
analysis %>%
  dplyr::select(-country_of_origin, -Qualityclass) %>%
  my_skim2() %>%
  dplyr::select(-c(n_missing, complete_rate, skim_type)) %>%
  kable(col.names = c("Variable", "Mean", "SD", "Min.", "1st Q.", "Median",
                        "3rd Q.", "Max."), 
        caption = 'Summary statistics of continuous variables in the data set.',
        booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 9, latex_options = "HOLD_position")
```

```{r, eval = FALSE}

#Create table to show the number of batches and the proportion of good quality for each country.

my_skim <- skim_with(base = sfl(n = length))
my.analysis <- analysis %>%
  mutate(Qualityclassindicator = as.numeric(Qualityclass=="Good"))
my.analysis %>%
  group_by(country_of_origin) %>%
  dplyr::select(country_of_origin, Qualityclassindicator) %>%
  my_skim() %>%
  dplyr::select(country_of_origin, n, numeric.mean) %>%
  transmute(country_of_origin=country_of_origin,
            number_of_batch=n,
            Proportion_of_good_quality=numeric.mean) %>%
  kable(caption = '\\label{tab:countryskim} Summary statistics of the sepal length by species of irises', booktabs = TRUE, linesep = "", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

The following boxplot is for good quality rates for each country, in which we can check if any countries have unusual high or low good quality rate.
```{r , echo=FALSE, fig.width = 6, fig.align = "center", fig.cap = "Boxplots of good quality rate for each country.", fig.pos = 'H', message = FALSE}

analysis.country <- analysis %>%
  mutate(Qualityclassindicator = as.numeric(Qualityclass=="Good")) %>%
  group_by(country_of_origin) %>%
  dplyr::summarise(good_quality_porprotion = round(mean(Qualityclassindicator), 2),
            number_of_batch = n())

ggplot(data = analysis.country, mapping = aes(y = good_quality_porprotion)) +
  geom_boxplot()



```

The following table filter countries and its number of batch with 20% good quality rate before and after, which provides more detailed information than the above boxplot. The number of batch can imply the reliability. For instance, Colombia has a relatively high good quality rate with large number of batch. 
```{r, eval = TRUE}

analysis.country %>%
  filter(good_quality_porprotion <= quantile(good_quality_porprotion, probs = c(0.2, 0.8))[1] |
          good_quality_porprotion >= quantile(good_quality_porprotion, probs = c(0.2, 0.8))[2] ) %>%
  arrange(good_quality_porprotion) %>%
  kable(caption = '\\label{tab:countryodd} Origins with twenty percent good quality rate before and after', booktabs = TRUE,
        linesep ="",format = "latex", digits = 2) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")

```

The following table is the distribution of features between coffee in good and poor quality. We can check if there is any obvious difference in some features.

```{r, eval = TRUE}
my_skim <- skim_with(base = sfl(n = length))
#Create table showing numerical summaries for aroma, flavor, acidity, and category_two_defects separately for batches rates as good and poor for their quality.

analysis %>%
  group_by(Qualityclass) %>%
  dplyr::select(Qualityclass, aroma, flavor, acidity, category_two_defects, altitude_mean_meters, harvested) %>%
  my_skim() %>%
  transmute(Variable=skim_variable, Qualityclass=Qualityclass, n=n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(caption = '\\label{tab:catskim} Summary statistics of the sepal length by species of irises', booktabs = TRUE, linesep = "", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

```{r ,eval=FALSE, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Boxplots of countinous features on different quality class.", fig.pos = 'H', message = FALSE}
analysis.box <- analysis %>%
  filter(aroma != 0)


p1 <- ggplot(data = analysis.box, mapping = aes(x = factor(Qualityclass), y = aroma)) +
  geom_boxplot() +
  labs(x = 'Quality Class')
p2 <- ggplot(data = analysis.box, mapping = aes(x = factor(Qualityclass), y = flavor)) +
  geom_boxplot() +
  labs(x = 'Quality Class')
p3 <- ggplot(data = analysis.box, mapping = aes(x = factor(Qualityclass), y = acidity)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p4 <- ggplot(data = analysis.box, mapping = aes(x = factor(Qualityclass), y = harvested)) +
  geom_boxplot()+
  labs(x = 'Quality Class')

grid.arrange(p1, p2, p3, p4, ncol = 4)

```

```{r aroma, eval = FALSE, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for aroma.", fig.pos = 'H', message = FALSE}

#Generate histogram to visualize aroma data.

p1 <- ggplot(data = analysis, mapping = aes(x = aroma)) +
  geom_histogram()

#Generate boxplot to visualize aroma data.

p2 <- ggplot(data = analysis, mapping = aes(y = aroma)) +
  geom_boxplot()

grid.arrange(p1, p2, ncol = 2)

#Single observation with a value of 0 is odd.
```

```{r flavor,eval=FALSE, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for flavor.", fig.pos = 'H', message = FALSE}

#Generate histogram to visualize flavor data.

p1 <- ggplot(data = analysis, mapping = aes(x = flavor)) +
  geom_histogram()

#Generate boxplot to visualize flavor data.

p2 <- ggplot(data = analysis, mapping = aes(y = flavor)) +
  geom_boxplot()
  
grid.arrange(p1, p2, ncol = 2)

# Single observation with a value of 0 is odd.
```


```{r acidity,eval=FALSE, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for acidity.", fig.pos = 'H', message = FALSE}

#Generate histogram to visualize acidity data.

p1 <- ggplot(data = analysis, mapping = aes(x = acidity)) +
  geom_histogram()

#Generate boxplot to visualize acidity data.

p2 <- ggplot(data = analysis, mapping = aes(y = acidity)) +
geom_boxplot()
  
grid.arrange(p1, p2, ncol = 2)

# Single observation with a value of 0 is odd. See if this same observation has a value of 0 for flavor and aroma as well.

analysis %>% 
  filter(acidity == 0 & flavor == 0 & aroma == 0)
  
#Will delete the observation in the formal analysis. 
```

```{r category_two_defects,eval=FALSE, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for category two defects.", fig.pos = 'H', message = FALSE}

#Generate histogram to visualize category_two_defects data.

p1 <- ggplot(data = analysis, mapping = aes(x = category_two_defects)) +
  geom_histogram()

#Generate boxplot to visualize category_two_defects data.

p2 <- ggplot(data = analysis, mapping = aes(y = category_two_defects)) +
  geom_boxplot()
  
grid.arrange(p1, p2, ncol = 2)

#The data is right skewed, so a log-transformation will be applied.
```

```{r category_two_defects2,eval=FALSE, echo = FALSE}

# Apply log(x+1) transformation and then visualize the data set.

analysis1 <- analysis %>%
  mutate(defects_log = log(category_two_defects+1))

glimpse(analysis1)
  
```

``` {r category_two_defects3,eval = FALSE, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for category two defects after log transformation.", fig.pos = 'H', message = FALSE}

# Generate histogram to visualize category_two_defects data after log-transformation.

p1 <- ggplot(data = analysis1, mapping = aes(x = defects_log)) +
  geom_histogram()

# Generate boxplot to visualize category_two_defects data after log-transformation.

p2 <- ggplot(data = analysis1, mapping = aes(y = defects_log)) +
  geom_boxplot()

grid.arrange(p1, p2, ncol = 2)
```


``` {r category_two_defects4, eval = FALSE, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram for category two defects and its log transformation.", fig.pos = 'H', message = FALSE}
# The following two hists can show the effect of log-transformation on counts of type-2 defects
# Generate histogram to visualize category_two_defects data after log-transformation.

p1 <- ggplot(data = analysis1, mapping = aes(x = category_two_defects)) +
  geom_histogram(color = 'white')

# Generate boxplot to visualize category_two_defects data after log-transformation.

p2 <- ggplot(data = analysis1, mapping = aes(x = defects_log)) +
  geom_histogram(color = 'white')

grid.arrange(p1, p2, ncol = 2)
```

``` {r altitude, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for altitude.", fig.pos = 'H', message = FALSE}

# Generate histogram to visualize altitude data.

p1 <- ggplot(data = analysis, mapping = aes(x = altitude_mean_meters)) +
  geom_histogram()

# Generate boxplot to visualize altitude data.

p2 <- ggplot(data = analysis, mapping = aes(y = altitude_mean_meters)) +
geom_boxplot()

grid.arrange(p1, p2, ncol = 2)
```
There are several observations with extremly high altitude which are impossible. Hence, delete observations which have altitude higher than Mt. Everest.

```{r altitude2, echo = FALSE}

# Mt. Everest is only 8,849 meters tall. Remove any observations with altitudes higher than that.

analysis2 <- analysis %>%
  filter(altitude_mean_meters < 8849)
```

``` {r altitude3, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for altitude after removing implausuble observations.", fig.pos = 'H', message = FALSE}

# Generate histogram to visualize altitude data after removing implausible observations.

p1 <- ggplot(data = analysis2, mapping = aes(x = altitude_mean_meters)) +
  geom_histogram()

# Generate boxplot to visualize altitude data after removing implausible observations.

p2 <- ggplot(data = analysis2, mapping = aes(y = altitude_mean_meters)) +
  geom_boxplot()

grid.arrange(p1, p2, ncol = 2)
```

The following two histograms comparing distributions of altitude before and after removing implausible observations.

``` {r altitude4, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram for altitude befor and after removing implausuble observations.", fig.pos = 'H', message = FALSE}

# Generate histogram to visualize altitude data after removing implausible observations.

p1 <- ggplot(data = analysis, mapping = aes(x = altitude_mean_meters)) +
  geom_histogram(color = 'white')

# Generate boxplot to visualize altitude data after removing implausible observations.

p2 <- ggplot(data = analysis2, mapping = aes(x = altitude_mean_meters)) +
  geom_histogram(color = 'white')

grid.arrange(p1, p2, ncol = 2)
```

``` {r harvested,eval = FALSE, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for harvested.", fig.pos = 'H', message = FALSE}

# Generate histogram to visualize year of harvest data.

p1 <- ggplot(data = analysis, mapping = aes(x = harvested)) +
  geom_histogram()

# Generate boxplot to visualize year of harvest data.

p2 <- ggplot(data = analysis, mapping = aes(y = harvested)) +
  geom_boxplot()

grid.arrange(p1, p2, ncol = 2)
```

The following table is the distribution of featers between good and poor coffee. We can check if there is obvious difference in some features between good and poor coffee.

```{r, eval = TRUE}
#Create table showing numerical summaries for aroma, flavor, acidity, and category_two_defects separately for batches rates as good and poor for their quality.

analysis2 %>%
  group_by(Qualityclass) %>%
  dplyr::select(Qualityclass, aroma, flavor, acidity, category_two_defects, altitude_mean_meters, harvested) %>%
  my_skim() %>%
  transmute(Variable=skim_variable, Qualityclass=Qualityclass, n=n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(caption = '\\label{tab:catskim} Summary statistics of features of good and poor coffee', booktabs = TRUE, linesep = "", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

Here is 6 box-plots comparing features distribution between good and poor coffee.

```{r , echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Boxplots2 of countinous features on different quality class.", fig.pos = 'H', message = FALSE}
analysis2.box <- analysis2 %>%
  filter(aroma != 0)


p1 <- ggplot(data = analysis2.box, mapping = aes(x = factor(Qualityclass), y = aroma)) +
  geom_boxplot() +
  labs(x = 'Quality Class')
p2 <- ggplot(data = analysis2.box, mapping = aes(x = factor(Qualityclass), y = flavor)) +
  geom_boxplot() +
  labs(x = 'Quality Class')
p3 <- ggplot(data = analysis2.box, mapping = aes(x = factor(Qualityclass), y = acidity)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p4 <- ggplot(data = analysis2.box, mapping = aes(x = factor(Qualityclass), y = category_two_defects)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p5 <- ggplot(data = analysis2.box, mapping = aes(x = factor(Qualityclass), y = altitude_mean_meters)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p6 <- ggplot(data = analysis2.box, mapping = aes(x = factor(Qualityclass), y = harvested)) +
  geom_boxplot()+
  labs(x = 'Quality Class')

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 6)

```

```{r, echo = FALSE}

# glimpse(analysis)

# Create a new data set with the proportion of batches that were good and the number of observations.  

analysis3 <- analysis %>%
  mutate(Qualityclass_indicator = 0 + as.numeric(Qualityclass=="Good")) %>%
  group_by(country_of_origin) %>%
  summarize(proportion = mean(Qualityclass_indicator), n=n()) %>%
  arrange(desc(proportion))

# View(analysis3)
```

```{r country, eval=FALSE, echo = FALSE}
#Create a bar chart to visualize the proportion of batches that were good for each country. Split up the visualizations into four groups. 

p1<-ggplot(data = analysis3[1:9,], mapping = aes(x = country_of_origin, y = proportion)) +
  geom_col() + geom_text(data = analysis3[1:9,], aes(label = n), nudge_y=0.25)

p2<-ggplot(data = analysis3[10:18,], mapping = aes(x = country_of_origin, y = proportion)) +
  geom_col() + geom_text(data = analysis3[10:18,], aes(label = n), nudge_y=0.35)

p3<-ggplot(data = analysis3[19:26,], mapping = aes(x = country_of_origin, y = proportion)) +
  geom_col() + geom_text(data = analysis3[19:26,], aes(label = n), nudge_y=0.35)

p4<-ggplot(data = analysis3[27:34,], mapping = aes(x = country_of_origin, y = proportion)) +
  geom_col() + geom_text(data = analysis3[27:34,], aes(label = n), nudge_y=0.3)

grid.arrange(p1, p2, p3, p4, nrow = 4)
```

```{r boxplots, echo = FALSE}

# glimpse(analysis)

# Create the final data set. Remove observations that have implausible values for at least one of the variables. Standardize aroma, flavor, acidity, and altitude. Apply a log(x+1) transformation to category_two_defects variable. Turn Qualityclass into an indicator variable so that it can be used in the logistic regression analysis. Remove unneeded variables.

coffee_final <- analysis %>%
  na.omit() %>%
  filter(altitude_mean_meters < 8849) %>%
  mutate(year = as.factor(harvested), Qualityclass = 0 + as.numeric(Qualityclass == "Good")) 

##divide 3 levels for the altitude based on review of the literature @hameed_coffee_2020
coffee_final$level_1 <- ifelse(coffee_final$altitude_mean_meters<900,1,0)
coffee_final$level_2 <- ifelse(coffee_final$altitude_mean_meters>=900 & coffee_final$altitude_mean_meters<=1200,1,0)
coffee_final$level_3 <- ifelse(coffee_final$altitude_mean_meters>1200,1,0)
coffee_final <- coffee_final %>%
  mutate(level = as.character(level_1 + level_2*2 + level_3*3))

# Finally, create the final data set by removing any unncessary variables. 

coffee_final <- coffee_final %>%
  dplyr::select(country_of_origin, aroma, flavor, acidity, category_two_defects, year, level, Qualityclass)

# View the final data set and make sure that the standardizations and transformation were appropriately applied.

# glimpse(coffee_final)

# skim_without_charts(coffee_final)


```

```{r, eval=FALSE}

# We generate a table to have a look at the proportion and counts for both Good and Poor quality based on the country of origin and harvested years respectively.

coffee_final %>% 
  tabyl(Qualityclass, country_of_origin) %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting() %>% 
  adorn_ns()

coffee_final %>% 
  tabyl(Qualityclass, year) %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting() %>% 
  adorn_ns()
```

```{r, eval=FALSE}

# Plot a bar chart to get intuition of whether the quality of coffee is influenced by the year of harvesting.

ggplot(coffee_final, aes(x= year,  y = ..prop.., group=Qualityclass, fill=Qualityclass)) + 
    geom_bar(position="dodge", stat="count") +
    labs(y = "Proportion")
```

# Formal Analysis Using Logistic Regression

```{r}

#Fit model with no intercept and only altitude as a predictor to get an idea of how quality differs by altitude level.

model_level <- glm(Qualityclass ~ level - 1, data = coffee_final,family = binomial(link = "logit"))
summary(model_level)
```

If the level of altitude is the only explanatory variable in the model, the effect of three levels are all statistically significant. In detail, high altitude has a positive influence on the quality of coffee.

```{r}

#Fit model with only year as a predictor to get an idea of how quality differs by year of harvest.

model_year <- glm(Qualityclass ~ year - 1, data = coffee_final,family = binomial(link = "logit"))
summary(model_year)

coffee_final$year2010 <- ifelse(coffee_final$year == 2010,1,0)
coffee_final$year2011 <- ifelse(coffee_final$year == 2011,1,0)
coffee_final$year2012 <- ifelse(coffee_final$year == 2012,1,0)
```

If harvested year is the only explanatory variable in the model, the effects of year 2010, 2011 and 2012 are statistically significant. Coffee harvested in year 2012 has a higher odds ratio. Coffee harvested in year 2010 and 2011 has a lower odds ratio.

```{r}

#Fit model with only country as a predictor to get an idea of how quality differs by country of harvest.

model_country <- glm(Qualityclass ~ country_of_origin - 1, data = coffee_final,family = binomial(link = "logit"))
summary(model_country)
```

If the country of origin is the only explanatory variable, Colombia, Mexico, Honduras, Kenya, Malawi, Uganda have statistically significant effect on the odds ratio.

```{r}

#Create indicator variables for countries that appear to have a significant influence on the quality of a harvest. These will be used to simplify the model in later iterations. 

coffee_final$Colombia <- ifelse(coffee_final$country_of_origin == 'Colombia',1,0)
coffee_final$Mexico <- ifelse(coffee_final$country_of_origin == 'Mexico',1,0)
coffee_final$Honduras <- ifelse(coffee_final$country_of_origin == 'Honduras',1,0)
coffee_final$Kenya <- ifelse(coffee_final$country_of_origin == 'Kenya',1,0)
coffee_final$Malawi <- ifelse(coffee_final$country_of_origin == 'Malawi',1,0)
coffee_final$Uganda <- ifelse(coffee_final$country_of_origin == 'Uganda',1,0)
```

```{r}

#Fit model with new indicator variables for important countries to compare the quality between them.

model_co_4 <- glm(Qualityclass ~ Colombia + Mexico + Honduras + Kenya + Malawi + Uganda - 1, data = coffee_final,family = binomial(link = "logit"))
summary(model_co_4)
```

```{r, eval=FALSE}

#Fit model with only country and year.

model_cn_ye <- glm(Qualityclass ~ country_of_origin + year, data = coffee_final,family = binomial(link = "logit"))
summary(model_cn_ye)
```

```{r,warning=FALSE, eval=FALSE}

#Fit model with indicators for important countries and altitude level.

model_al_co <- glm(Qualityclass ~ level + Colombia + Mexico + Honduras + Kenya, data = coffee_final,family = binomial(link = "logit"))
summary(model_al_co)
```


```{r,warning=FALSE, eval=FALSE}

#Fit model with altitude level, country, and year with no intercept.

model_al_co <- glm(Qualityclass ~ level + country_of_origin + year - 1, data = coffee_final,family = binomial(link = "logit"))
summary(model_al_co)
```

The following is the model considering all possible expalntatory variables.
```{r}

#Fit logistic regression model with all possible predictors. Will need to be simplified given how many countries there are. 

model_all <- glm(Qualityclass ~ aroma + flavor + acidity + country_of_origin + category_two_defects + level + year, data = coffee_final,family = binomial(link = "logit"))
summary(model_all)
```

```{r, eval=FALSE}

#Create data set with incomplete observations (i.e., those missing any data) removed so that stepwise selection using AIC can be performed.

coffee_final_nomiss <- coffee_final %>%
  filter(!is.na(level) & !is.na(year))

glimpse(coffee_final_nomiss)

model_all_2 <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico + Honduras + Kenya + defects_log + level + year, data = coffee_final_nomiss, family = binomial(link = "logit"))
summary(model_all_2)

# I want to try could we use step AIC directly on the full model
model_all_4 <- glm(Qualityclass ~ aroma + flavor + acidity + country_of_origin + defects_log + level + year, data = coffee_final_nomiss,family = binomial(link = "logit"))
# However, the full model has warnings.
```



```{r, warning=FALSE}
model_all_significant <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico + Honduras + Kenya + Malawi + Uganda + category_two_defects + level + year2010 + year2011 + year2012, data = coffee_final,family = binomial(link = "logit"))

summary(model_all_significant)
#Find the model that best fits the data using AIC. Incomplete observations are dropped.

model_slected <- stepAIC(model_all_significant, direction = 'both')
summary(model_slected)
```

Firstly, we conduct a model with all significant explanatory variables and use step_AIC to select variables.
In the selected model, two terms are not significant. Then, we try to delete term Uganda which has the highest p-value.

```{r}
model2 <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico + category_two_defects, data = coffee_final,family = binomial(link = "logit"))
model3 <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico, data = coffee_final,family = binomial(link = "logit"))
summary(model2)
summary(model3)
anova(model_slected, model2, model3)
qchisq(df = 1, p = 0.95)
```

After deleting Uganda, category_two_defects is still not significant. Hence, it was deleted.
And we use anova to compare three models.
There isn't statistically significant difference among them.
Hence, it is reasonable to delete them and get a simple model.


```{r, eval=FALSE}

#Create data set with incomplete observations imputed to the most median value.

table(coffee_final$level)
table(coffee_final$year)

#Impute 3 for altitude level and 2015 (5) for year of harvest.

coffee_final_imputed <- coffee_final %>%
  mutate(level=ifelse(is.na(level),3,level), year=ifelse(is.na(year),5,year))

glimpse(coffee_final_imputed)
table(coffee_final_imputed$year)

#Fit logistic regression model using imputed values.

model_all_3 <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico + Honduras + Kenya + defects_log + level + year, data = coffee_final_imputed, family = binomial(link = "logit"))
summary(model_all_2)
```

```{r, eval=FALSE}

#Find the model that best fits the data using AIC. Incomplete observations are imputed to the median value.

stepAIC(model_all_3, direction = 'both')
```

##Final Model
```{r}
summary(model3)
```

$$\ln(\frac{p_i}{1-p_i}) = \alpha + \beta_1 \cdot aroma_i+ \beta_2 \cdot flavor_i+ \beta_3 \cdot acidity_i+ \beta_4 \cdot \mathbb{I}_{\mbox{Colombia}}(x)+ \beta_5 \cdot \mathbb{I}_{\mbox{Mexico}}(x)$$

$$\mathbb{I}_{\mbox{Colombia}}(x)=\left\{ \begin{array}{ll} 1 ~~~ \mbox{if Country of region of} ~ x \mbox{th observation is Colombia},\\ 0 ~~~ \mbox{Otherwise}.\\ \end{array} \right. $$

$$\mathbb{I}_{\mbox{Mexico}}(x)=\left\{ \begin{array}{ll} 1 ~~~ \mbox{if Country of region of} ~ x \mbox{th observation is Mexico},\\ 0 ~~~ \mbox{Otherwise}.\\ \end{array} \right.$$
The following is the fitted model.

$$\ln(\frac{p_i}{1-p_i}) = `r round(coef(model3), 2)[1]` + `r round(coef(model3), 2)[2]` \cdot aroma_i+ `r round(coef(model3), 2)[3]` \cdot flavor_i+ `r round(coef(model3), 2)[4]` \cdot acidity_i+ `r round(coef(model3), 2)[5]` \cdot \mathbb{I}_{\mbox{Colombia}}(x)+ `r round(coef(model3), 2)[6]` \cdot \mathbb{I}_{\mbox{Mexico}}(x)$$

```{r, eval=FALSE}
# I think maybe scale of these grades and give levels for altitudes are unnecessary.
# Hence I try to use them directly to fit the model and assess the model performance.
# Use the origin data can make the model more interpretable. (Enyu Li)

coffee_all_variables <- analysis %>%
  filter(acidity != 0 & (altitude_mean_meters < 8849 & !is.na(altitude_mean_meters)) & !is.na(harvested)) %>%
  mutate(defects_log = log(category_two_defects + 1), year = as.factor(harvested), Qualityclass = 0 + as.numeric(Qualityclass == "Good")) 

countrylist <- unique(coffee_all_variables$country_of_origin)
countrylist

for(i in 1:length(countrylist)){
  coffee_all_variables[,10+i] <- as.numeric(coffee_all_variables$country_of_origin == countrylist[i])
  colnames(coffee_all_variables)[10+i] <- countrylist[i]
}

# coffee_all_variables <- coffee_all_variables %>%
  # dplyr::select(-country_of_origin)

head(coffee_all_variables)

# I remove the intercept to test whether the slope of each country is statistically significant.
# With the intercept, the significant depneds on the baseline country.
model_all_variable <- glm(Qualityclass ~ country_of_origin - 1 , data = coffee_all_variables, family = binomial(link = "logit"))
summary(model_all_variable)

# Including those countries whose effect is significant.
model_all_variable2 <- glm(Qualityclass ~ aroma + flavor + acidity + altitude_mean_meters + harvested + defects_log + Colombia + Honduras + Kenya + Malawi + Mexico + Uganda , data = coffee_all_variables, family = binomial(link = "logit"))
summary(model_all_variable2)


```

```{r, eval=FALSE}

#Do variable selection by AIC

modelselection <- stepAIC(model_all_variable2, direction = 'both')
summary(modelselection)

# Note that the final model inclues Uganda which is not significant
# Removing it and compare 

model_all_variable3 <- glm(Qualityclass ~ aroma + flavor + acidity + defects_log + Colombia + Mexico
                           , data = coffee_all_variables, family = binomial(link = "logit"))

anova(modelselection, model_all_variable3)

# Not significant difference
# The same variable selection result with my peers

```

```{r, warning=FALSE}

#Perform 10-fold cross validation to assess the predictive performance of the model. Calculate overall accuracy, sensitivity, and specificity. 

set.seed(9)
folds <- createFolds(y=coffee_final$Qualityclass, k=10)
accuracy <- as.numeric()
sensitivity <- as.numeric()
specificity <- as.numeric()
for(i in 1:10){
  fold_test <- coffee_final[folds[[i]],]
  fold_train <- coffee_final[-folds[[i]],]
  fold_pre <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico,family = binomial(link = logit), data =fold_train )
  fold_predict <- predict(fold_pre,type='response',newdata=fold_test)
  fold_predict <- ifelse(fold_predict >= 0.5, 1, 0)
  accuracy[i] <- mean(fold_predict == fold_test[,8])
  sensitivity[i] <- sum(fold_predict + fold_test[,8] == 2) / sum(fold_test[,8] == 1)
  specificity[i] <- sum(fold_predict + fold_test[,8] == 0) / sum(fold_test[,8] == 0)
}
mean(accuracy)

mean(sensitivity)

mean(specificity)
```
The accuracy of our final model is `r round(mean(accuracy), 2)`.
The sensitivity of our final model is `r round(mean(sensitivity), 2)`.
The specificity of our final model is `r round(mean(specificity), 2)`.


```{r, eval=FALSE}

#Fit a linear mixed model with a random intercept for country. Included all other predictors from the final, best model.

glmm1 <- glmer(Qualityclass ~ 1 + aroma + flavor + acidity +  (1|country_of_origin),
data=coffee_final, family=binomial(link="logit"))
summary(glmm1, corr=FALSE)
```

```{r, eval=FALSE}

#Perform 10-fold cross validation to assess the predictive performance of the model without country (i.e., predictions based on linear mixed model). Calculate overall accuracy, sensitivity, and specificity. 

set.seed(9)
folds <- createFolds(y=coffee_final$Qualityclass, k=10)
accuracy <- as.numeric()
sensitivity <- as.numeric()
specificity <- as.numeric()
for(i in 1:10){
  fold_test <- coffee_final[folds[[i]],]
  fold_train <- coffee_final[-folds[[i]],]
  fold_pre <- glm(Qualityclass ~ aroma + flavor + acidity + defects_log,family = binomial(link = logit), data =fold_train )
  fold_predict <- predict(fold_pre,type='response',newdata=fold_test)
  fold_predict <- ifelse(fold_predict >= 0.5, 1, 0)
  accuracy[i] <- mean(fold_predict == fold_test[,8])
  sensitivity[i] <- sum(fold_predict + fold_test[,8] == 2) / sum(fold_test[,8] == 1)
  specificity[i] <- sum(fold_predict + fold_test[,8] == 0) / sum(fold_test[,8] == 0)
}
mean(accuracy)

mean(sensitivity)

mean(specificity)

#Predictive accuracy is still very good. 
```

## Classfication boundry
```{r}
coffee_final$prid <- predict(model3, coffee_final, type='response')
score <- prediction(coffee_final$prid, coffee_final$Qualityclass)
perf <- performance(score, 'tpr', 'fpr')
auc <- performance(score, 'auc')
perfd <- data.frame(x=perf@x.values[1][[1]], y=perf@y.values[1][[1]])
ggplot(perfd, aes(x= x, y=y)) + geom_line() +
  xlab("False positive rate") + ylab("True positive rate") +
  ggtitle(paste("Area under the curve:", round(auc@y.values[[1]], 3)))

```


```{r, eval=FALSE}
cutoffs <- data.frame(cut=perf@alpha.values[[1]], 
                      fpr=perf@x.values[[1]], 
                      tpr=perf@y.values[[1]],
                      dif=perf@y.values[[1]] - perf@x.values[[1]])
head(cutoffs[order(cutoffs$dif, decreasing = T), ])
bestcut <- cutoffs[order(cutoffs$dif, decreasing = T), ]$cut[1]
bestcut

```

```{r, eval=FALSE}
cutoffs <- data.frame(cut=perf@alpha.values[[1]], 
                      fpr=perf@x.values[[1]], 
                      tpr=perf@y.values[[1]],
                      dif=perf@y.values[[1]] - perf@x.values[[1]])
head(cutoffs[order(cutoffs$dif, decreasing = T), ])


```

```{r, eval=FALSE}

#Perform 10-fold cross validation to assess the predictive performance of the model which use the best cutoff point by the whole dataset.
# Calculate overall accuracy, sensitivity, and specificity. 

set.seed(9)
folds <- createFolds(y=coffee_final$Qualityclass, k=10)
accuracy <- as.numeric()
sensitivity <- as.numeric()
specificity <- as.numeric()
for(i in 1:10){
  fold_test <- coffee_final[folds[[i]],]
  fold_train <- coffee_final[-folds[[i]],]
  fold_pre <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico + defects_log,family = binomial(link = logit), data =fold_train )
  fold_predict <- predict(fold_pre,type='response',newdata=fold_test)
  fold_predict <- ifelse(fold_predict >= bestcut, 1, 0)
  accuracy[i] <- mean(fold_predict == fold_test[,8])
  sensitivity[i] <- sum(fold_predict + fold_test[,8] == 2) / sum(fold_test[,8] == 1)
  specificity[i] <- sum(fold_predict + fold_test[,8] == 0) / sum(fold_test[,8] == 0)
}
mean(accuracy)

mean(sensitivity)

mean(specificity)
```

```{r}
# Here is a function which can find the best cutoff point automatically
findbestcut <- function(model, data){
  data$prid <- predict(model, data, type='response')
  score <- prediction(data$prid, data$Qualityclass)
  perf <- performance(score, 'tpr', 'fpr')
  cutoffs <- data.frame(cut=perf@alpha.values[[1]], dif=perf@y.values[[1]] - perf@x.values[[1]])
  bestcut <- cutoffs[order(cutoffs$dif, decreasing = T), ]$cut[1]
  return(bestcut)
}

```

```{r, warning=FALSE}

#Perform 10-fold cross validation to assess the predictive performance of the model which can use best cutoff point automatically.
#Calculate overall accuracy, sensitivity, and specificity. 

set.seed(9)
folds <- createFolds(y=coffee_final$Qualityclass, k=10)
accuracy <- as.numeric()
sensitivity <- as.numeric()
specificity <- as.numeric()
for(i in 1:10){
  fold_test <- coffee_final[folds[[i]],]
  fold_train <- coffee_final[-folds[[i]],]
  fold_pre <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico,family = binomial(link = logit), data =fold_train )
  fold_predict <- predict(fold_pre,type='response',newdata=fold_test)
  fold_predict <- ifelse(fold_predict >= findbestcut(fold_pre, fold_train), 1, 0)
  accuracy[i] <- mean(fold_predict == fold_test[,8])
  sensitivity[i] <- sum(fold_predict + fold_test[,8] == 2) / sum(fold_test[,8] == 1)
  specificity[i] <- sum(fold_predict + fold_test[,8] == 0) / sum(fold_test[,8] == 0)
}
mean(accuracy)

mean(sensitivity)

mean(specificity)
# The accuracy and sensitivity be improved a little bit.
```
After adjusting the classfication boundry.
The accuracy of our final model is `r round(mean(accuracy), 2)`.
The sensitivity of our final model is `r round(mean(sensitivity), 2)`.
The specificity of our final model is `r round(mean(specificity), 2)`.


```{r, eval=FALSE}

# Assess the preformance of the model using grades without scale. (Enyu Li)

set.seed(9)
folds <- createFolds(y=coffee_all_variables$Qualityclass, k=10)
accuracy <- as.numeric()
sensitivity <- as.numeric()
specificity <- as.numeric()
for(i in 1:10){
  fold_test <- coffee_all_variables[folds[[i]],]
  fold_train <- coffee_all_variables[-folds[[i]],]
  fold_pre <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico + defects_log + Uganda,family = binomial(link = logit), data =fold_train )
  fold_predict <- predict(fold_pre,type='response',newdata=fold_test)
  fold_predict <- ifelse(fold_predict >= findbestcut(fold_pre, fold_train), 1, 0)
  accuracy[i] <- mean(fold_predict == fold_test[,8])
  sensitivity[i] <- sum(fold_predict + fold_test[,8] == 2) / sum(fold_test[,8] == 1)
  specificity[i] <- sum(fold_predict + fold_test[,8] == 0) / sum(fold_test[,8] == 0)
}
mean(accuracy)

mean(sensitivity)

mean(specificity)
# Similar results with the model created with my peers
# According to the simplicity and interpretability of the model, above model is more proper. (Enyu Li)
```


```{r, eval=FALSE}

#Fit a linear mixed model with a random intercept for country. Included all other predictors from the final, best model.

glmm1 <- glmer(Qualityclass ~ 1 + aroma + flavor + acidity + defects_log + (1|country_of_origin),
               data=coffee_all_variables, family=binomial(link="logit"))
summary(glmm1, corr=FALSE)
```
