---
title: "Group_9_Analysis"
author: "Brent Strong, Enyu Li, Haotian Wang, Honjin Ren, Mu He"
date: "3/7/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, message = FALSE, warning = FALSE)
```


```{r libraries, echo = FALSE}

#Load necessary libraries for the data exploration and analysis

library(tidyverse)
library(moderndive)
library(skimr)
library(kableExtra)
library(gridExtra)
library(broom)
library(olsrr)
library(gapminder)
library(sjPlot)
library(stats)
library(jtools)
library(MASS)
library(janitor)
library(ggplot2)
library(caret)
library(lme4)
library(ROCR)
```

```{r import, echo = FALSE}

#Read in data from github and abbreviate certain country names.

analysis <- read_csv("https://raw.githubusercontent.com/brent-strong/DAS2022-Group-09/main/dataset9.csv")

for(i in 1:nrow(analysis)){
  if(str_detect(analysis$country_of_origin[i], "Puerto Rico")){
    analysis$country_of_origin[i] <- "Puerto Rico"
  }
  if(str_detect(analysis$country_of_origin[i], "Hawaii")){
    analysis$country_of_origin[i] <- "Hawaii"
  }
  if(str_detect(analysis$country_of_origin[i], "Tanzania")){
    analysis$country_of_origin[i] <- "Tanzania"
  }
  else{
    analysis$country_of_origin[i] <- analysis$country_of_origin[i]
  }
}

```

# Exploratory Data Analysis

Have a look of summary statistics of raw data
```{r, echo=FALSE, eval = TRUE}

#Create table of summary statistics for each of the continuous variables. 

my_skim2 <- skim_with(numeric = sfl(hist = NULL))
analysis %>%
  dplyr::select(-country_of_origin, -Qualityclass) %>%
  my_skim2() %>%
  dplyr::select(-c(n_missing, complete_rate, skim_type)) %>%
  kable(col.names = c("Variable", "Mean", "SD", "Min.", "1st Q.", "Median",
                        "3rd Q.", "Max."), 
        caption = 'Summary statistics of continuous variables in the data set.',
        booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 9, latex_options = "HOLD_position")
```


The following table is the number of batches and the proportion of good quality for each country.
```{r}

#Create table to show the number of batches and the proportion of good quality for each country.

my_skim <- skim_with(base = sfl(n = length))
my.analysis <- analysis %>%
  mutate(Qualityclassindicator = as.numeric(Qualityclass=="Good"))
my.analysis %>%
  group_by(country_of_origin) %>%
  dplyr::select(country_of_origin, Qualityclassindicator) %>%
  my_skim() %>%
  dplyr::select(country_of_origin, n, numeric.mean) %>%
  transmute(country_of_origin=country_of_origin,
            number_of_batch=n,
            Proportion_of_good_quality=numeric.mean) %>%
  kable(caption = '\\label{tab:countryskim} Summary statistics of the sepal length by species of irises', booktabs = TRUE, linesep = "", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

The following boxplot is for good quality rates for each country, in which we can check if any countries have unusual high or low good quality rate. It seems like all good quality rates lie in IQR.
```{r , echo=FALSE, fig.width = 6, fig.align = "center", fig.cap = "Boxplots of good quality rate for each country.", fig.pos = 'H', message = FALSE}

analysis.country <- analysis %>%
  mutate(Qualityclassindicator = as.numeric(Qualityclass=="Good")) %>%
  group_by(country_of_origin) %>%
  dplyr::summarise(good_quality_porprotion = round(mean(Qualityclassindicator), 2),
            number_of_batch = n())

ggplot(data = analysis.country, mapping = aes(y = good_quality_porprotion)) +
  geom_boxplot()



```

The following table filter countries and its number of batch with 20% good quality rate before and after, which provides more detailed information than the above boxplot. The number of batch can imply the reliability. For instance, Colombia has a relatively high good quality rate with large number of batch. 
```{r, eval = TRUE}

analysis.country %>%
  filter(good_quality_porprotion <= quantile(good_quality_porprotion, probs = c(0.2, 0.8))[1] |
          good_quality_porprotion >= quantile(good_quality_porprotion, probs = c(0.2, 0.8))[2] ) %>%
  arrange(good_quality_porprotion) %>%
  kable(caption = '\\label{tab:countryodd} Origins with twenty percent good quality rate before and after', booktabs = TRUE,
        linesep ="",format = "latex", digits = 2) %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")

```

The following table is the distribution of features between coffee in good and poor quality. We can check if there is any obvious difference in some features.

```{r, eval = TRUE}
my_skim <- skim_with(base = sfl(n = length))
#Create table showing numerical summaries for aroma, flavor, acidity, and category_two_defects separately for batches rates as good and poor for their quality.

analysis %>%
  group_by(Qualityclass) %>%
  dplyr::select(Qualityclass, aroma, flavor, acidity, category_two_defects, altitude_mean_meters, harvested) %>%
  my_skim() %>%
  transmute(Variable=skim_variable, Qualityclass=Qualityclass, n=n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(caption = '\\label{tab:catskim} Summary statistics of the sepal length by species of irises', booktabs = TRUE, linesep = "", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

```{r , echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Boxplots2 of countinous features on different quality class.", fig.pos = 'H', message = FALSE}

p1 <- ggplot(data = analysis, mapping = aes(x = factor(Qualityclass), y = aroma)) +
  geom_boxplot() +
  labs(x = 'Quality Class')
p2 <- ggplot(data = analysis, mapping = aes(x = factor(Qualityclass), y = flavor)) +
  geom_boxplot() +
  labs(x = 'Quality Class')
p3 <- ggplot(data = analysis, mapping = aes(x = factor(Qualityclass), y = acidity)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p4 <- ggplot(data = analysis, mapping = aes(x = factor(Qualityclass), y = category_two_defects)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p5 <- ggplot(data = analysis, mapping = aes(x = factor(Qualityclass), y = altitude_mean_meters)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p6 <- ggplot(data = analysis, mapping = aes(x = factor(Qualityclass), y = harvested)) +
  geom_boxplot()+
  labs(x = 'Quality Class')

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 6)

```
There are several observations with extremly high altitude which are impossible. Hence, delete observations which have altitude higher than Mt. Everest.

```{r altitude2, echo = FALSE}

# Mt. Everest is only 8,849 meters tall. Remove any observations with altitudes higher than that.

analysis2 <- analysis %>%
  filter(altitude_mean_meters < 8849)
```

``` {r altitude3, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram and boxplot for altitude after removing implausuble observations.", fig.pos = 'H', message = FALSE}

# Generate histogram to visualize altitude data after removing implausible observations.

p1 <- ggplot(data = analysis2, mapping = aes(x = altitude_mean_meters)) +
  geom_histogram()

# Generate boxplot to visualize altitude data after removing implausible observations.

p2 <- ggplot(data = analysis2, mapping = aes(y = altitude_mean_meters)) +
  geom_boxplot()

grid.arrange(p1, p2, ncol = 2)
```

The following two histograms comparing distributions of altitude before and after removing implausible observations.

``` {r altitude4, echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Histogram for altitude befor and after removing implausuble observations.", fig.pos = 'H', message = FALSE}

# Generate histogram to visualize altitude data after removing implausible observations.

p1 <- ggplot(data = analysis, mapping = aes(x = altitude_mean_meters)) +
  geom_histogram(color = 'white')

# Generate boxplot to visualize altitude data after removing implausible observations.

p2 <- ggplot(data = analysis2, mapping = aes(x = altitude_mean_meters)) +
  geom_histogram(color = 'white')

grid.arrange(p1, p2, ncol = 2)
```


The following table is the distribution of featers between good and poor coffee. We can check if there is obvious difference in some features between good and poor coffee after data cleaning.

```{r, eval = TRUE}
#Create table showing numerical summaries for aroma, flavor, acidity, and category_two_defects separately for batches rates as good and poor for their quality.

analysis2 %>%
  group_by(Qualityclass) %>%
  dplyr::select(Qualityclass, aroma, flavor, acidity, category_two_defects, altitude_mean_meters, harvested) %>%
  my_skim() %>%
  transmute(Variable=skim_variable, Qualityclass=Qualityclass, n=n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(caption = '\\label{tab:catskim} Summary statistics of features of good and poor coffee', booktabs = TRUE, linesep = "", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```


```{r boxplots, echo = FALSE}

# glimpse(analysis)

# Create the final data set. Remove observations that have implausible values for at least one of the variables. Standardize aroma, flavor, acidity, and altitude. Apply a log(x+1) transformation to category_two_defects variable. Turn Qualityclass into an indicator variable so that it can be used in the logistic regression analysis. Remove unneeded variables.

coffee_final <- analysis %>%
  na.omit() %>%
  filter(altitude_mean_meters < 8849) %>%
  mutate(year = as.factor(harvested), Qualityclass = 0 + as.numeric(Qualityclass == "Good")) 

##divide 3 levels for the altitude based on review of the literature @hameed_coffee_2020
coffee_final$level_1 <- ifelse(coffee_final$altitude_mean_meters<900,1,0)
coffee_final$level_2 <- ifelse(coffee_final$altitude_mean_meters>=900 & coffee_final$altitude_mean_meters<=1200,1,0)
coffee_final$level_3 <- ifelse(coffee_final$altitude_mean_meters>1200,1,0)
coffee_final <- coffee_final %>%
  mutate(level = as.character(level_1 + level_2*2 + level_3*3))

# Finally, create the final data set by removing any unncessary variables. 

coffee_final <- coffee_final %>%
  dplyr::select(country_of_origin, aroma, flavor, acidity, category_two_defects, year, level, Qualityclass)

# View the final data set and make sure that the standardizations and transformation were appropriately applied.

# glimpse(coffee_final)

# skim_without_charts(coffee_final)


```

Here is 6 box-plots comparing features distribution between good and poor coffee after data cleaning

```{r , echo=FALSE, fig.width = 13, fig.align = "center", fig.cap = "Boxplots of countinous features on different quality class after data cleaning.", fig.pos = 'H', message = FALSE}
coffee_final2 <- analysis %>%
  na.omit() %>%
  filter(altitude_mean_meters < 8849)


p1 <- ggplot(data = coffee_final2, mapping = aes(x = factor(Qualityclass), y = aroma)) +
  geom_boxplot() +
  labs(x = 'Quality Class')
p2 <- ggplot(data = coffee_final2, mapping = aes(x = factor(Qualityclass), y = flavor)) +
  geom_boxplot() +
  labs(x = 'Quality Class')
p3 <- ggplot(data = coffee_final2, mapping = aes(x = factor(Qualityclass), y = acidity)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p4 <- ggplot(data = coffee_final2, mapping = aes(x = factor(Qualityclass), y = category_two_defects)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p5 <- ggplot(data = coffee_final2, mapping = aes(x = factor(Qualityclass), y = altitude_mean_meters)) +
  geom_boxplot()+
  labs(x = 'Quality Class')
p6 <- ggplot(data = coffee_final2, mapping = aes(x = factor(Qualityclass), y = harvested)) +
  geom_boxplot()+
  labs(x = 'Quality Class')

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 6)

```

# Formal Analysis Using Logistic Regression

Firstly we fit a model using altitude levels as the only explanatory variable.
```{r}

#Fit model with no intercept and only altitude as a predictor to get an idea of how quality differs by altitude level.

model_level <- glm(Qualityclass ~ level - 1, data = coffee_final,family = binomial(link = "logit"))
summary(model_level)
```

If the level of altitude is the only explanatory variable in the model, the effect of three levels are all statistically significant. In detail, high altitude has a positive influence on the quality of coffee.

Secondly, we fit a model using harvested year as the only explanatory variable.

```{r}

#Fit model with only year as a predictor to get an idea of how quality differs by year of harvest.

model_year <- glm(Qualityclass ~ year - 1, data = coffee_final,family = binomial(link = "logit"))
summary(model_year)

coffee_final$year2010 <- ifelse(coffee_final$year == 2010,1,0)
coffee_final$year2011 <- ifelse(coffee_final$year == 2011,1,0)
coffee_final$year2012 <- ifelse(coffee_final$year == 2012,1,0)
```

If harvested year is the only explanatory variable in the model, the effects of year 2010, 2011 and 2012 are statistically significant. Coffee harvested in year 2012 has a higher odds ratio. Coffee harvested in year 2010 and 2011 has a lower odds ratio.

Then, we fit a model using country of region as the only explanatory variable.
```{r}

#Fit model with only country as a predictor to get an idea of how quality differs by country of harvest.

model_country <- glm(Qualityclass ~ country_of_origin - 1, data = coffee_final,family = binomial(link = "logit"))
summary(model_country)
```

If the country of origin is the only explanatory variable, Colombia, Mexico, Honduras, Kenya, Malawi, Uganda have statistically significant effect on the odds ratio.

```{r}

#Create indicator variables for countries that appear to have a significant influence on the quality of a harvest. These will be used to simplify the model in later iterations. 

coffee_final$Colombia <- ifelse(coffee_final$country_of_origin == 'Colombia',1,0)
coffee_final$Mexico <- ifelse(coffee_final$country_of_origin == 'Mexico',1,0)
coffee_final$Honduras <- ifelse(coffee_final$country_of_origin == 'Honduras',1,0)
coffee_final$Kenya <- ifelse(coffee_final$country_of_origin == 'Kenya',1,0)
coffee_final$Malawi <- ifelse(coffee_final$country_of_origin == 'Malawi',1,0)
coffee_final$Uganda <- ifelse(coffee_final$country_of_origin == 'Uganda',1,0)
```

All variables which are significant above are considered to be potential explanatory variables. They are all three altitude levels, year 2010, 2011 and 2012 and countries of Colombia, Mexico, Honduras, Kenya, Malawi and Uganda.

The following model use all potential  explanatory variables. And use step AIC to select variables again.
```{r, warning=FALSE}
model_all_significant <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico + Honduras + Kenya + Malawi + Uganda + category_two_defects + level + year2010 + year2011 + year2012, data = coffee_final,family = binomial(link = "logit"))

summary(model_all_significant)
#Find the model that best fits the data using AIC. Incomplete observations are dropped.

model_slected <- stepAIC(model_all_significant, direction = 'both')
summary(model_slected)
```

In the selected model, two terms are not significant. Then, we try to delete term Uganda which has the highest p-value.

After deleting Uganda, category_two_defects is still not significant. Hence, it was deleted.
And we use anova to compare three models.
There isn't statistically significant difference among them.
Hence, it is reasonable to delete them and get a simple model.

```{r}
model2 <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico + category_two_defects, data = coffee_final,family = binomial(link = "logit"))
model3 <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico, data = coffee_final,family = binomial(link = "logit"))
summary(model2)
summary(model3)
anova(model_slected, model2, model3)
qchisq(df = 1, p = 0.95)
```

## Final Model
```{r}
summary(model3)
```

$$\ln(\frac{p_i}{1-p_i}) = \alpha + \beta_1 \cdot aroma_i+ \beta_2 \cdot flavor_i+ \beta_3 \cdot acidity_i+ \beta_4 \cdot \mathbb{I}_{\mbox{Colombia}}(x)+ \beta_5 \cdot \mathbb{I}_{\mbox{Mexico}}(x)$$

$$\mathbb{I}_{\mbox{Colombia}}(x)=\left\{ \begin{array}{ll} 1 ~~~ \mbox{if Country of region of} ~ x \mbox{th observation is Colombia},\\ 0 ~~~ \mbox{Otherwise}.\\ \end{array} \right. $$

$$\mathbb{I}_{\mbox{Mexico}}(x)=\left\{ \begin{array}{ll} 1 ~~~ \mbox{if Country of region of} ~ x \mbox{th observation is Mexico},\\ 0 ~~~ \mbox{Otherwise}.\\ \end{array} \right.$$
The following is the fitted model.

$$\ln(\frac{p_i}{1-p_i}) = `r round(coef(model3), 2)[1]` + `r round(coef(model3), 2)[2]` \cdot aroma_i+ `r round(coef(model3), 2)[3]` \cdot flavor_i+ `r round(coef(model3), 2)[4]` \cdot acidity_i+ `r round(coef(model3), 2)[5]` \cdot \mathbb{I}_{\mbox{Colombia}}(x) `r round(coef(model3), 2)[6]` \cdot \mathbb{I}_{\mbox{Mexico}}(x)$$

Generate a summary table containing confidence intervals of estimated parameters of final model.

```{r}
modelsumm <- summary(model3)
CI <- confint(model3)
citable <- data.frame(modelsumm$coefficients) %>%
  round(2)
colnames(citable)[1] <- "Estimate" 
colnames(citable)[2] <- "Std error"
colnames(citable)[3] <- "statistic"
colnames(citable)[4] <- "P value"
citable$Lower_ci <- CI[,1]
citable$Upper_ci <- CI[,2]
citable$Est_exp <- exp(citable$Estimate)
citable$Lower_ci_exp <- exp(CI[,1])
citable$Upper_ci_exp <- exp(CI[,2])
citable <- citable[,-3]
knitr::kable(citable, caption = '\\label{tab:ci} confidence interval of estimated parameters', booktabs = TRUE, linesep = "", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

Based on the model we built, we try to use 10-folds cross validation to test the validity of our final model. In the validation we prefer three criteria: accuracy, sensitivity and specificity.

```{r, warning=FALSE}

#Perform 10-fold cross validation to assess the predictive performance of the model. Calculate overall accuracy, sensitivity, and specificity. 

set.seed(9)
folds <- createFolds(y=coffee_final$Qualityclass, k=10)
accuracy <- as.numeric()
sensitivity <- as.numeric()
specificity <- as.numeric()
for(i in 1:10){
  fold_test <- coffee_final[folds[[i]],]
  fold_train <- coffee_final[-folds[[i]],]
  fold_pre <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico,family = binomial(link = logit), data =fold_train )
  fold_predict <- predict(fold_pre,type='response',newdata=fold_test)
  fold_predict <- ifelse(fold_predict >= 0.5, 1, 0)
  accuracy[i] <- mean(fold_predict == fold_test[,8])
  sensitivity[i] <- sum(fold_predict + fold_test[,8] == 2) / sum(fold_test[,8] == 1)
  specificity[i] <- sum(fold_predict + fold_test[,8] == 0) / sum(fold_test[,8] == 0)
}
mean(accuracy)

mean(sensitivity)

mean(specificity)
```
The accuracy of our final model is `r round(mean(accuracy), 2)`.
The sensitivity of our final model is `r round(mean(sensitivity), 2)`.
The specificity of our final model is `r round(mean(specificity), 2)`.

## Classfication boundry

```{r}
coffee_final$prid <- predict(model3, coffee_final, type='response')
score <- prediction(coffee_final$prid, coffee_final$Qualityclass)
perf <- performance(score, 'tpr', 'fpr')
auc <- performance(score, 'auc')
perfd <- data.frame(x=perf@x.values[1][[1]], y=perf@y.values[1][[1]])
ggplot(perfd, aes(x= x, y=y)) + geom_line() +
  xlab("False positive rate") + ylab("True positive rate") +
  ggtitle(paste("Area under the curve:", round(auc@y.values[[1]], 3))) + 
  geom_abline(intercept = 0, slope = 1, linetype = 2, color = 'red')

```

```{r}
round(coef(model3),2)
round(exp(coef(model3)),2)
co <- rbind(round(coef(model3),2),round(exp(coef(model3)),2))
row.names(co) <- c("coefficients","exp(coefficients)")



co %>%
  knitr::kable(caption = '\\label{tab:summaries} Regression coefficients and exponentiated coefficients.', booktabs = TRUE, linesep = "", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")

```

```{r}
plot_model(model3, show.values = TRUE,
           title = "Log-Odds", show.p = FALSE)
```


```{r}
# Here is a function which can find the best cutoff point automatically
findbestcut <- function(model, data){
  data$prid <- predict(model, data, type='response')
  score <- prediction(data$prid, data$Qualityclass)
  perf <- performance(score, 'tpr', 'fpr')
  cutoffs <- data.frame(cut=perf@alpha.values[[1]], dif=perf@y.values[[1]] - perf@x.values[[1]])
  bestcut <- cutoffs[order(cutoffs$dif, decreasing = T), ]$cut[1]
  return(bestcut)
}

```

```{r, warning=FALSE}

#Perform 10-fold cross validation to assess the predictive performance of the model which can use best cutoff point automatically.
#Calculate overall accuracy, sensitivity, and specificity. 

set.seed(9)
folds <- createFolds(y=coffee_final$Qualityclass, k=10)
accuracy <- as.numeric()
sensitivity <- as.numeric()
specificity <- as.numeric()
for(i in 1:10){
  fold_test <- coffee_final[folds[[i]],]
  fold_train <- coffee_final[-folds[[i]],]
  fold_pre <- glm(Qualityclass ~ aroma + flavor + acidity + Colombia + Mexico,family = binomial(link = logit), data =fold_train )
  fold_predict <- predict(fold_pre,type='response',newdata=fold_test)
  fold_predict <- ifelse(fold_predict >= findbestcut(fold_pre, fold_train), 1, 0)
  accuracy[i] <- mean(fold_predict == fold_test[,8])
  sensitivity[i] <- sum(fold_predict + fold_test[,8] == 2) / sum(fold_test[,8] == 1)
  specificity[i] <- sum(fold_predict + fold_test[,8] == 0) / sum(fold_test[,8] == 0)
}
mean(accuracy)

mean(sensitivity)

mean(specificity)
# The accuracy and sensitivity be improved a little bit.
```
After adjusting the classification boundary.
The accuracy of our final model is `r round(mean(accuracy), 2)`.
The sensitivity of our final model is `r round(mean(sensitivity), 2)`.
The specificity of our final model is `r round(mean(specificity), 2)`.


In addition, we try the linear mix model.
However, it doesn't improve the performance of predicting.
```{r, eval=FALSE}

#Fit a linear mixed model with a random intercept for country. Included all other predictors from the final, best model.

glmm1 <- glmer(Qualityclass ~ 1 + aroma + flavor + acidity + defects_log + (1|country_of_origin),
               data=coffee_all_variables, family=binomial(link="logit"))
summary(glmm1, corr=FALSE)
```
